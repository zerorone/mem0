#!/usr/bin/env python3
"""
Mem0 GLM MCP Server
åŸºäº mem0 å’Œ GLM çš„æ™ºèƒ½è®°å¿†ç®¡ç† MCP æœåŠ¡å™¨
"""

import os
import sys
import json
import asyncio
import logging
from typing import Dict, List, Optional, Any
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from fastmcp import FastMCP
    from fastmcp.exceptions import FastMCPError
except ImportError:
    print("âŒ FastMCP æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install fastmcp")
    sys.exit(1)

try:
    from mem0 import Memory
    from mem0.configs.base import MemoryConfig
    from mem0.configs.llms.glm import GLMConfig
    from mem0.llms.glm import GLMLLM
    from mem0.llms.configs import LlmConfig
    from mem0.embeddings.configs import EmbedderConfig
    from mem0.vector_stores.configs import VectorStoreConfig
    from mem0.graphs.configs import GraphStoreConfig
except ImportError as e:
    print(f"âŒ Mem0 æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('mem0_mcp_server.log')
    ]
)
logger = logging.getLogger(__name__)

class Mem0GLMConfig:
    """Mem0 GLM MCP æœåŠ¡å™¨é…ç½®ç®¡ç†"""
    
    def __init__(self):
        self.config = self._load_config()
        self._validate_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """æŒ‰ä¼˜å…ˆçº§åŠ è½½é…ç½®æ–‡ä»¶"""
        config_paths = [
            os.path.expanduser("~/.claude/mem0-glm-config.json"),
            "config/development.json",
            "config/.env.json"
        ]
        
        for config_path in config_paths:
            if os.path.exists(config_path):
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                    logger.info(f"âœ… åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
                    return config
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ {config_path}: {e}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡
        logger.warning("âš ï¸  æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡")
        return self._load_from_env()
    
    def _load_from_env(self) -> Dict[str, Any]:
        """ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®"""
        return {
            # GLM é…ç½®
            "glm_api_key": os.getenv("GLM_API_KEY", "91654c7966f149dc94f4bdcba1d90fa3.BGlbzom7iMDHyjhS"),
            "glm_base_url": os.getenv("GLM_BASE_URL", "https://open.bigmodel.cn/api/paas/v4"),
            "glm_model": os.getenv("GLM_MODEL", "glm-4.5"),
            
            # Vector Store é…ç½®
            "vector_store_provider": os.getenv("VECTOR_STORE_PROVIDER", "chroma"),
            "vector_store_config": {
                "collection_name": os.getenv("COLLECTION_NAME", "mem0_memories"),
                "path": os.getenv("CHROMA_PATH", "/tmp/chroma_mem0")
            },
            
            # åµŒå…¥æ¨¡å‹é…ç½®
            "embedding_provider": os.getenv("EMBEDDING_PROVIDER", "ollama"),
            "embedding_config": {
                "model": os.getenv("EMBEDDING_MODEL", "bge-m3")
            },
            
            # æ—¥å¿—é…ç½®
            "log_level": os.getenv("LOG_LEVEL", "INFO"),
            "enable_thinking": os.getenv("ENABLE_THINKING", "true").lower() == "true"
        }
    
    def _validate_config(self):
        """éªŒè¯é…ç½®"""
        required_keys = ["glm_api_key", "vector_store_provider"]
        for key in required_keys:
            if not self.config.get(key):
                logger.error(f"âŒ ç¼ºå°‘å¿…éœ€é…ç½®é¡¹: {key}")
                raise ValueError(f"Missing required config: {key}")
        
        logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")

class Mem0GLMServer:
    """Mem0 GLM MCP æœåŠ¡å™¨ä¸»ç±»"""
    
    def __init__(self):
        self.config_manager = Mem0GLMConfig()
        self.memory = None
        self._init_memory()
    
    def _init_memory(self):
        """åˆå§‹åŒ– Mem0 å®ä¾‹"""
        try:
            # åˆ›å»º LLM é…ç½®
            llm_config = LlmConfig(
                provider="glm",
                config={
                    "model": self.config_manager.config["glm_model"],
                    "api_key": self.config_manager.config["glm_api_key"],
                    "glm_base_url": self.config_manager.config.get("glm_base_url"),
                    "temperature": 0.7,
                    "enable_thinking": self.config_manager.config.get("enable_thinking", True)
                }
            )
            
            # åˆ›å»ºå‘é‡å­˜å‚¨é…ç½®
            vector_store_config = VectorStoreConfig(
                provider=self.config_manager.config["vector_store_provider"],
                config=self.config_manager.config.get("vector_store_config", {})
            )
            
            # åˆ›å»ºåµŒå…¥æ¨¡å‹é…ç½®
            embedder_config = EmbedderConfig(
                provider=self.config_manager.config.get("embedding_provider", "ollama"),
                config=self.config_manager.config.get("embedding_config", {})
            )
            
            # åˆ›å»ºå›¾æ•°æ®åº“é…ç½®ï¼ˆå¯é€‰ï¼‰
            graph_store_config = None
            if self.config_manager.config.get("graph_store_provider"):
                graph_store_config = GraphStoreConfig(
                    provider=self.config_manager.config["graph_store_provider"],
                    config=self.config_manager.config.get("graph_store_config", {})
                )
            
            # åˆ›å»ºå®Œæ•´çš„ MemoryConfig å¯¹è±¡
            memory_config = MemoryConfig(
                llm=llm_config,
                vector_store=vector_store_config,
                embedder=embedder_config,
                graph_store=graph_store_config if graph_store_config else GraphStoreConfig(),
                version="v1.1"
            )
            
            # åˆå§‹åŒ– Memory å®ä¾‹
            self.memory = Memory(memory_config)
            logger.info("âœ… Mem0 å®ä¾‹åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ Mem0 å®ä¾‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.memory = None
            # åˆ›å»ºä¸€ä¸ªåŸºç¡€çš„ GLM å®ä¾‹ä»¥æä¾›åŸºæœ¬åŠŸèƒ½
            try:
                glm_config = GLMConfig(
                    model=self.config_manager.config["glm_model"],
                    api_key=self.config_manager.config["glm_api_key"],
                    glm_base_url=self.config_manager.config.get("glm_base_url"),
                    enable_thinking=self.config_manager.config.get("enable_thinking", True)
                )
                self.glm_llm = GLMLLM(glm_config)
                logger.info("âœ… å¤‡ç”¨ GLM LLM åˆå§‹åŒ–æˆåŠŸ")
            except Exception as glm_error:
                logger.error(f"âŒ GLM LLM åˆå§‹åŒ–ä¹Ÿå¤±è´¥: {glm_error}")
                self.glm_llm = None

# åˆ›å»ºæœåŠ¡å™¨å®ä¾‹
server_instance = Mem0GLMServer()

# åˆ›å»º MCP åº”ç”¨
mcp = FastMCP("Mem0-GLM Memory Server")

@mcp.tool()
async def add_memory(
    content: str,
    user_id: str = "default_user",
    metadata: Optional[Dict[str, Any]] = None
) -> str:
    """
    æ·»åŠ è®°å¿†åˆ° Mem0
    
    Args:
        content: è¦è®°å¿†çš„å†…å®¹
        user_id: ç”¨æˆ·IDï¼Œé»˜è®¤ä¸º default_user
        metadata: å¯é€‰çš„å…ƒæ•°æ®
    
    Returns:
        æ·»åŠ ç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        if not server_instance.memory:
            raise FastMCPError("INTERNAL_ERROR", "Mem0 å®ä¾‹ä¸å¯ç”¨")
        
        result = server_instance.memory.add(
            content, 
            user_id=user_id, 
            metadata=metadata or {}
        )
        
        logger.info(f"âœ… æˆåŠŸæ·»åŠ è®°å¿† - ç”¨æˆ·: {user_id}")
        return json.dumps({
            "success": True,
            "message": "è®°å¿†æ·»åŠ æˆåŠŸ",
            "result": result
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"âŒ æ·»åŠ è®°å¿†å¤±è´¥: {e}")
        return json.dumps({
            "success": False,
            "error": str(e)
        }, ensure_ascii=False)

@mcp.tool()
async def search_memories(
    query: str,
    user_id: str = "default_user",
    limit: int = 10
) -> str:
    """
    æœç´¢è®°å¿†
    
    Args:
        query: æœç´¢æŸ¥è¯¢
        user_id: ç”¨æˆ·ID
        limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
    
    Returns:
        æœç´¢ç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        if not server_instance.memory:
            raise FastMCPError("INTERNAL_ERROR", "Mem0 å®ä¾‹ä¸å¯ç”¨")
        
        results = server_instance.memory.search(
            query, 
            user_id=user_id, 
            limit=limit
        )
        
        logger.info(f"âœ… æœç´¢è®°å¿†æˆåŠŸ - ç”¨æˆ·: {user_id}, æŸ¥è¯¢: {query}")
        return json.dumps({
            "success": True,
            "query": query,
            "user_id": user_id,
            "results": results
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"âŒ æœç´¢è®°å¿†å¤±è´¥: {e}")
        return json.dumps({
            "success": False,
            "error": str(e)
        }, ensure_ascii=False)

@mcp.tool()
async def get_all_memories(
    user_id: str = "default_user"
) -> str:
    """
    è·å–ç”¨æˆ·çš„æ‰€æœ‰è®°å¿†
    
    Args:
        user_id: ç”¨æˆ·ID
    
    Returns:
        æ‰€æœ‰è®°å¿†çš„JSONå­—ç¬¦ä¸²
    """
    try:
        if not server_instance.memory:
            raise FastMCPError("INTERNAL_ERROR", "Mem0 å®ä¾‹ä¸å¯ç”¨")
        
        memories = server_instance.memory.get_all(user_id=user_id)
        
        logger.info(f"âœ… è·å–æ‰€æœ‰è®°å¿†æˆåŠŸ - ç”¨æˆ·: {user_id}")
        return json.dumps({
            "success": True,
            "user_id": user_id,
            "count": len(memories) if memories else 0,
            "memories": memories
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"âŒ è·å–è®°å¿†å¤±è´¥: {e}")
        return json.dumps({
            "success": False,
            "error": str(e)
        }, ensure_ascii=False)

@mcp.tool()
async def update_memory(
    memory_id: str,
    content: str,
    user_id: str = "default_user"
) -> str:
    """
    æ›´æ–°æŒ‡å®šè®°å¿†
    
    Args:
        memory_id: è®°å¿†ID
        content: æ–°çš„å†…å®¹
        user_id: ç”¨æˆ·ID
    
    Returns:
        æ›´æ–°ç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        if not server_instance.memory:
            raise FastMCPError("INTERNAL_ERROR", "Mem0 å®ä¾‹ä¸å¯ç”¨")
        
        result = server_instance.memory.update(
            memory_id, 
            content, 
            user_id=user_id
        )
        
        logger.info(f"âœ… æ›´æ–°è®°å¿†æˆåŠŸ - ID: {memory_id}")
        return json.dumps({
            "success": True,
            "message": "è®°å¿†æ›´æ–°æˆåŠŸ",
            "memory_id": memory_id,
            "result": result
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°è®°å¿†å¤±è´¥: {e}")
        return json.dumps({
            "success": False,
            "error": str(e)
        }, ensure_ascii=False)

@mcp.tool()
async def delete_memory(
    memory_id: str,
    user_id: str = "default_user"
) -> str:
    """
    åˆ é™¤æŒ‡å®šè®°å¿†
    
    Args:
        memory_id: è®°å¿†ID
        user_id: ç”¨æˆ·ID
    
    Returns:
        åˆ é™¤ç»“æœçš„JSONå­—ç¬¦ä¸²
    """
    try:
        if not server_instance.memory:
            raise FastMCPError("INTERNAL_ERROR", "Mem0 å®ä¾‹ä¸å¯ç”¨")
        
        server_instance.memory.delete(memory_id, user_id=user_id)
        
        logger.info(f"âœ… åˆ é™¤è®°å¿†æˆåŠŸ - ID: {memory_id}")
        return json.dumps({
            "success": True,
            "message": "è®°å¿†åˆ é™¤æˆåŠŸ",
            "memory_id": memory_id
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"âŒ åˆ é™¤è®°å¿†å¤±è´¥: {e}")
        return json.dumps({
            "success": False,
            "error": str(e)
        }, ensure_ascii=False)

@mcp.tool()
async def chat_with_memory(
    message: str,
    user_id: str = "default_user",
    include_memories: bool = True
) -> str:
    """
    åŸºäºè®°å¿†çš„æ™ºèƒ½å¯¹è¯
    
    Args:
        message: ç”¨æˆ·æ¶ˆæ¯
        user_id: ç”¨æˆ·ID
        include_memories: æ˜¯å¦åŒ…å«ç›¸å…³è®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡
    
    Returns:
        å¯¹è¯å“åº”çš„JSONå­—ç¬¦ä¸²
    """
    try:
        # å¦‚æœæœ‰ Mem0 å®ä¾‹ï¼Œå…ˆæœç´¢ç›¸å…³è®°å¿†
        context_memories = []
        if server_instance.memory and include_memories:
            try:
                memories = server_instance.memory.search(message, user_id=user_id, limit=5)
                context_memories = [m.get('memory', '') for m in memories if m.get('memory')]
            except Exception as mem_error:
                logger.warning(f"âš ï¸  æœç´¢è®°å¿†æ—¶å‡ºé”™: {mem_error}")
        
        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        if context_memories:
            context = f"ç›¸å…³è®°å¿†:\n" + "\n".join([f"- {mem}" for mem in context_memories])
            full_message = f"{context}\n\nç”¨æˆ·é—®é¢˜: {message}"
        else:
            full_message = message
        
        # ä½¿ç”¨ GLM ç”Ÿæˆå“åº”
        if server_instance.glm_llm:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥åŸºäºç”¨æˆ·çš„å†å²è®°å¿†æä¾›å¸®åŠ©ã€‚"},
                {"role": "user", "content": full_message}
            ]
            
            response = server_instance.glm_llm.generate_response(messages)
        else:
            response = "æŠ±æ­‰ï¼ŒGLM æœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆå“åº”ã€‚"
        
        logger.info(f"âœ… å¯¹è¯æˆåŠŸ - ç”¨æˆ·: {user_id}")
        return json.dumps({
            "success": True,
            "message": message,
            "response": response,
            "context_memories_count": len(context_memories)
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"âŒ å¯¹è¯å¤±è´¥: {e}")
        return json.dumps({
            "success": False,
            "error": str(e)
        }, ensure_ascii=False)

@mcp.tool()
async def memory_stats(
    user_id: str = "default_user"
) -> str:
    """
    è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        user_id: ç”¨æˆ·ID
    
    Returns:
        ç»Ÿè®¡ä¿¡æ¯çš„JSONå­—ç¬¦ä¸²
    """
    try:
        stats = {
            "user_id": user_id,
            "mem0_available": server_instance.memory is not None,
            "glm_available": server_instance.glm_llm is not None
        }
        
        if server_instance.memory:
            try:
                all_memories = server_instance.memory.get_all(user_id=user_id)
                stats["total_memories"] = len(all_memories) if all_memories else 0
            except Exception:
                stats["total_memories"] = "æ— æ³•è·å–"
        
        logger.info(f"âœ… è·å–ç»Ÿè®¡ä¿¡æ¯æˆåŠŸ - ç”¨æˆ·: {user_id}")
        return json.dumps(stats, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return json.dumps({
            "success": False,
            "error": str(e)
        }, ensure_ascii=False)

@mcp.tool()
async def health_check() -> str:
    """
    å¥åº·æ£€æŸ¥
    
    Returns:
        å¥åº·çŠ¶æ€çš„JSONå­—ç¬¦ä¸²
    """
    health_status = {
        "timestamp": "2025-09-02 12:36:01",
        "server_status": "running",
        "mem0_status": "available" if server_instance.memory else "unavailable",
        "glm_status": "available" if server_instance.glm_llm else "unavailable",
        "config_loaded": bool(server_instance.config_manager.config),
        "version": "1.0.0"
    }
    
    logger.info("âœ… å¥åº·æ£€æŸ¥å®Œæˆ")
    return json.dumps(health_status, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    logger.info("ğŸš€ å¯åŠ¨ Mem0-GLM MCP æœåŠ¡å™¨...")
    logger.info(f"âœ… æœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆ")
    logger.info(f"âœ… Mem0 çŠ¶æ€: {'å¯ç”¨' if server_instance.memory else 'ä¸å¯ç”¨'}")
    logger.info(f"âœ… GLM çŠ¶æ€: {'å¯ç”¨' if server_instance.glm_llm else 'ä¸å¯ç”¨'}")
    mcp.run()